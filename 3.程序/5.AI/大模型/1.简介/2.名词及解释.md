# 大模型领域常见名词及解释

以下是大模型领域常见名词及解释，按类别分类便于理解：

---

## **1. 核心架构与技术**

- **Transformer**  
  - 基于自注意力机制的神经网络架构，替代传统RNN/CNN，擅长处理长序列数据，成为大模型的基础（如GPT、BERT）。
- **自注意力（Self-Attention）**  
  - 通过计算序列中每个元素与其他元素的相关性权重，捕捉全局依赖关系。
- **多头注意力（Multi-Head Attention）**  
  - 将自注意力机制并行拆分多组，分别学习不同维度的特征，增强模型表达能力。

---

## **2. 训练与优化**

- **预训练（Pre-training）**  
  - 在大规模无标签数据上训练模型，学习通用表征（如GPT-3在万亿级文本训练）。
- **微调（Fine-tuning）**  
  - 在预训练模型基础上，用特定领域数据调整参数以适应下游任务（如医疗问答模型）。
- **提示学习（Prompt Learning）**  
  - 通过设计输入模板（如“这句话的情感是：__”）引导模型生成目标输出，减少微调数据需求。
- **强化学习人类反馈（RLHF）**  
  - 通过人类对模型输出的评分优化模型，提升对齐性（如ChatGPT的训练关键步骤）。

---

## **3. 模型类型与扩展**

- **生成式AI（Generative AI）**  
  - 生成文本、图像等内容的大模型（如GPT-4、DALL·E）。
- **多模态模型（Multimodal Model）**  
  - 同时处理文本、图像、语音等多种输入（如GPT-4V、Gemini）。
- **稀疏专家模型（MoE, Mixture of Experts）**  
  - 将模型拆分为多个专家子网络，动态激活部分参数提升效率（如Switch Transformer）。

---

## **4. 效率优化技术**

- **模型量化（Quantization）**  
  - 将模型参数从32位浮点压缩至8位整数，减少内存占用（如手机端部署LLM）。
- **低秩适应（LoRA）**  
  - 通过低秩矩阵微调部分参数，大幅降低训练成本（如微调Llama-2仅需1%显存）。
- **模型蒸馏（Knowledge Distillation）**  
  - 用小模型模仿大模型输出，实现轻量化（如DistilBERT保留BERT 95%性能，体积缩小40%）。

---

## **5. 评估与伦理**

- **困惑度（Perplexity）**  
  - 衡量语言模型预测能力的指标，值越低表示预测越准确。
- **AI对齐（AI Alignment）**  
  - 确保模型目标与人类价值观一致（如避免生成有害内容）。
- **幻觉（Hallucination）**  
  - 模型生成与事实不符的内容（如虚构历史事件）。

---

## **6. 硬件与部署**

- **模型并行（Model Parallelism）**  
  - 将超大模型拆分到多个GPU上训练（如Megatron-Turing的530B参数需数千块GPU）。
- **显存墙（Memory Wall）**  
  - 模型参数量增长速度远超显存容量提升，限制单卡训练能力。

---

## **典型模型示例**

- **GPT系列**：基于Transformer的自回归文本生成模型（如ChatGPT）。
- **BERT**：通过掩码语言建模预训练的双向Transformer模型。
- **T5**：将各类NLP任务统一为文本到文本的生成式框架。

---

以上术语涵盖了大模型的核心技术、训练方法及实际应用中的关键概念，可作为深入学习的知识框架基础。需要进一步解释某个方向可随时提出。
